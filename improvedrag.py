# -*- coding: utf-8 -*-
"""improvedRAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a8V-wFOfhEp8tb1yzfeFcrxDE4DZcHXC
"""

!pip install sentence-transformers
!pip install PyMuPDF
!pip install -U bitsandbytes

import torch
from sentence_transformers import SentenceTransformer, util
import nltk
import re
import fitz
from tqdm import tqdm
from typing import List, Tuple, Optional, Iterator
from dataclasses import dataclass
from transformers import AutoTokenizer, AutoModelForCausalLM, GPT2TokenizerFast, BitsAndBytesConfig
import textwrap
from transformers.utils import is_flash_attn_2_available
import logging
from pathlib import Path
import pickle
import gdown

@dataclass
class TextChunk:
  """Data class for storing text chunks as string"""
  pagenumber: int
  content: str
  token_count: int

class TextProcessor:
  """Cleans a page of text from pdf"""
  def __init__(self):
    nltk.download("punkt", quiet=True)
    self.tokenizer = GPT2TokenizerFast.from_pretrained("gpt2")

  def format_text(self, text: str) -> str:
    """Clean and format text"""
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'^\s*\d+|\d+\s*$', '', text, flags=re.MULTILINE)
    text = re.sub(r'^.*(?:Header|Footer).*$', '', text, flags=re.MULTILINE)
    return text.strip()

  def split_into_sentences(self, text: str) -> List[str]:
    """uses nltk to split into sentences"""
    return nltk.sent_tokenize(text)

  def count_tokens(self, text: str) -> int:
    """Count tokens in text"""
    return len(self.tokenizer.encode(text))

class PDFProcessor:
  """Reads in a pdf; formats it and then creates chunks"""
  def __init__(self, text_processor: TextProcessor, chunk_size: int = 10, overlap_size: int = 2, min_token_count: int = 20):
    self.text_processor = text_processor
    self.chunk_size = chunk_size
    self.overlap_size = overlap_size
    self.min_token_count = min_token_count

  def process_pdf(self, pdf_path: str, start_page: int = 0) -> Iterator[TextChunk]:
    """processes pdf in a streaming manner"""
    logging.info(f"Processing PDF: {pdf_path}")
    doc = fitz.open(pdf_path)
    buffer = []
    current_page = start_page

    for page_number in range(start_page, len(doc)):
      text = doc[page_number].get_text()
      clean_text = self.text_processor.format_text(text)

      if not clean_text:
        continue

      sentences = self.text_processor.split_into_sentences(clean_text)
      buffer.extend(sentences)

      while len(buffer) >= self.chunk_size:
        chunk_sentences = buffer[:self.chunk_size]
        buffer = buffer[self.chunk_size - self.overlap_size:]

        chunk_text = "".join(chunk_sentences).strip()
        chunk_text = re.sub(r'\.([A-Z])', r'. \1', chunk_text)
        token_count = self.text_processor.count_tokens(chunk_text)

        if token_count >= self.min_token_count:
          yield TextChunk(pagenumber=page_number,
                          content=chunk_text,
                          token_count = token_count)
      current_page = page_number + 1

      #process anything that was left in buffer
      if buffer:
        chunk_text = "".join(buffer).strip()
        chunk_text = re.sub(r'\.([A-Z])', r'. \1', chunk_text)
        token_count = self.text_processor.count_tokens(chunk_text)
        if token_count >= self.min_token_count:
          yield TextChunk(pagenumber=page_number,
                          content=chunk_text,
                          token_count = token_count)
    doc.close()

class Embedder:
  """Creates embedding for chunks"""
  def __init__(self, embedding_model_name: str = "all-mpnet-base-v2",
               device: str = "cuda" if torch.cuda.is_available() else "cpu"):
    self.device = device
    self.emmbedding_model = SentenceTransformer(model_name_or_path=embedding_model_name, device=self.device)

  def compute_embeddings(self, chunks: List[TextChunk], batch_size: int =32) -> torch.Tensor:
    """converts text to embeddings"""
    text = [chunk.content for chunk in chunks]
    embeddings = self.emmbedding_model.encode(text, batch_size=batch_size, convert_to_tensor=True)
    return embeddings.to(torch.float32).to(self.device) #should this be float32?

  def compute_query_embedding(self, query: str) -> torch.Tensor:
    """Converts user query to embedding"""
    return self.emmbedding_model.encode(query, convert_to_tensor=True)

class Retriever:
  """Retrieve similar embeddings"""
  def __init__(self, embedder: Embedder):
    self.embedder = embedder

  def get_similar_content(self, query: str,
                          document_embeddings: torch.Tensor,
                          chunks: List[TextChunk],
                          n_results: int = 5):
    """Rerieves content similar to query"""
    query_embedding = self.embedder.compute_query_embedding(query)
    dot_product = util.dot_score(query_embedding, document_embeddings)[0]
    _, indices = torch.topk(dot_product, n_results)
    return [chunks[i].content for i in indices]

class PromptManager:
    """Handles prompt construction and formatting"""
    def __init__(self):
        self.system_prompt = """Please answer the user's question based on the provided context. Extract relevant information from the context to form your answer. Do not include any of your thought processes in the final answer. Keep your response to 2 or 3 sentences."""

    def format_prompt(self, question: str, context_items: List[str]) -> str:
        """Format prompt with question and context"""
        context = "- " + "\n- ".join(context_items)
        """Based on the following context items, please answer the query.
Give yourself room to think by extracting relevant passages from the context before answering the query.
Don't return the thinking, only return the answer.
Make sure your answers are as explanatory as possible.
Use the following examples as reference for the ideal answer style.
\nExample 1:
Query: What are the fat-soluble vitamins?
Answer: The fat-soluble vitamins include Vitamin A, Vitamin D, Vitamin E, and Vitamin K. These vitamins are absorbed along with fats in the diet and can be stored in the body's fatty tissue and liver for later use. Vitamin A is important for vision, immune function, and skin health. Vitamin D plays a critical role in calcium absorption and bone health. Vitamin E acts as an antioxidant, protecting cells from damage. Vitamin K is essential for blood clotting and bone metabolism.
\nExample 2:
Query: What are the causes of type 2 diabetes?
Answer: Type 2 diabetes is often associated with overnutrition, particularly the overconsumption of calories leading to obesity. Factors include a diet high in refined sugars and saturated fats, which can lead to insulin resistance, a condition where the body's cells do not respond effectively to insulin. Over time, the pancreas cannot produce enough insulin to manage blood sugar levels, resulting in type 2 diabetes. Additionally, excessive caloric intake without sufficient physical activity exacerbates the risk by promoting weight gain and fat accumulation, particularly around the abdomen, further contributing to insulin resistance.
\nExample 3:
Query: What is the importance of hydration for physical performance?
Answer: Hydration is crucial for physical performance because water plays key roles in maintaining blood volume, regulating body temperature, and ensuring the transport of nutrients and oxygen to cells. Adequate hydration is essential for optimal muscle function, endurance, and recovery. Dehydration can lead to decreased performance, fatigue, and increased risk of heat-related illnesses, such as heat stroke. Drinking sufficient water before, during, and after exercise helps ensure peak physical performance and recovery.
\nNow use the following context items to answer the user query:
{context}
\nRelevant passages: <extract relevant passages from the context here>
User query: {query}
Answer:"""

        return prompt

    def set_system_prompt(self, new_system_prompt: str) -> None:
        self.system_prompt = new_system_prompt

class PromptManager:
  def __init__(self):
      self.system_prompt = """Based on the following context items, please answer the query.
Give yourself room to think by extracting relevant passages from the context before answering the query.
Don't return the thinking, only return the answer.
Make sure your answers are as explanatory as possible.
Use the following examples as reference for the ideal answer style.
\nExample 1:
Query: What are the major functions of water-soluble vitamins?
Answer: Water-soluble vitamins are essential nutrients that perform critical roles in maintaining body functions and health. They participate in energy metabolism through B-vitamins acting as coenzymes, facilitate growth and development via folate's role in DNA synthesis, support red blood cell formation through vitamin B12, and provide antioxidant protection through vitamin C. These vitamins are crucial for daily cellular processes and must be regularly consumed as they aren't stored in large amounts in the body.
\nExample 2:
Query: What are the functions of the kidneys?
Answer: The kidneys are vital organs that maintain the body's internal balance through several critical functions. They filter blood and remove waste products, regulate blood volume and pressure through complex hormonal mechanisms, maintain electrolyte balance essential for cellular function, produce important hormones like EPO for red blood cell production and calcitriol for calcium absorption, and help maintain proper acid-base balance in the blood. These functions work together to ensure optimal body function and homeostasis.
\nExample 3:
Query: What is imagery rehearsal therapy (IRT)?
Answer: Imagery rehearsal therapy is a therapeutic technique specifically designed to treat recurring nightmares, particularly in individuals with PTSD. This cognitive approach involves a structured process where patients work with therapists to identify and recall their nightmares, create new versions with less disturbing content, and practice visualizing these modified scenarios. The goal is to reduce the frequency and intensity of nightmares by giving patients more control over their dream content through conscious reimagining and rehearsal.
\nNow use the following context items to answer the user query:"""

  def set_system_prompt(self, new_system_prompt: str) -> None:
      self.system_prompt = new_system_prompt

  def format_prompt(self, question: str,
                     context_items: List[str]) -> str:


    context = "- " + "\n- ".join(context_items)

    prompt = f"""{self.system_prompt}

{context}
\nRelevant passages: <extract relevant passages from the context here>
User query: {question}
Answer:"""


    dialogue_template = [
        {"role": "user",
        "content": prompt}
    ]
    return dialogue_template

class LLM:
  """Manages language model and user interaction with the LLM"""
  def __init__(self, model_name: str = "google/gemma-7b-it",
               device: str = "cuda" if torch.cuda.is_available() else "cpu"):
    self.device = device
    self.tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_name)
    self.quantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_use_double_quant=True,
                                                              bnb_4bit_quant_type='nf4', bnb_4bit_compute_dtype=torch.float32)
    self.model = self._initializer(model_name)


  def _initializer(self, model_name: str) -> AutoModelForCausalLM:
    return AutoModelForCausalLM.from_pretrained(
                pretrained_model_name_or_path=model_name,
                device_map='auto',
                quantization_config=self.quantization_config,
                trust_remote_code=True
            )
  def generate_response(self, prompt: str,
                      max_tokens: int = 256,
                      temperature: float = 0.7) -> str:
    prompt = self.tokenizer.apply_chat_template(conversation=prompt,
                                          tokenize=False,
                                          add_generation_prompt=True)
    inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
    outputs = self.model.generate(**inputs,
                                temperature=temperature,
                                do_sample=True,
                                max_new_tokens=max_tokens)

    outputs = self.tokenizer.decode(outputs[0], skip_special_tokens=False)
    return outputs, prompt

class RAGPipeline:
  def __init__(self, embeddings, chunks, retriever, prompt_manager, llm):
    self.embeddings = embeddings
    self.chunks = chunks
    self.retriever = retriever
    self.prompt_manager = prompt_manager
    self.llm = llm

  def answer_question(self, question: str) -> None:
    similar_content = self.retriever.get_similar_content(question, self.embeddings, self.chunks)
    prompt = self.prompt_manager.format_prompt(question, similar_content)
    outputs, formatted_prompt = self.llm.generate_response(prompt)
    answer = outputs.replace(formatted_prompt,"").replace("<bos>", "").replace("<eos>", "")
    print(f"Question: {question}\nAnswer: {answer}")
    return

import warnings
warnings.filterwarnings("ignore")
def main():
  logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
  logger = logging.getLogger(__name__)

  llm = LLM()
  prompt_manager = PromptManager()

  Path('./books').mkdir(parents=True, exist_ok=True)
  gdown.download("https://drive.google.com/file/d/1ytXII7X3913MATiFj9-kbu8ygi1S8pmf/view?usp=drive_link", "./books/sleep.pdf",fuzzy=True, quiet = True)
  gdown.download("https://drive.google.com/file/d/1uuKy9ah2VhrjoX7x_k7ccmpz83rt0idg/view?usp=drive_link", "./books/anatomyPhysiology.pdf",fuzzy=True, quiet = True)
  gdown.download("https://drive.google.com/file/d/18AsZoeI6VeVHQp8CVgdBFOjNI-0DDOwm/view?usp=drive_link", "./books/humannutrition.pdf",fuzzy=True, quiet = True)

  text_processor = TextProcessor()
  pdf_processor = PDFProcessor(text_processor)

  pdf1 = pdf_processor.process_pdf("./books/sleep.pdf", 11)
  pdf2 = pdf_processor.process_pdf("./books/anatomyPhysiology.pdf", 23)
  pdf3 = pdf_processor.process_pdf("./books/humannutrition.pdf", 41)

  pdf1_chunks = [chunk for chunk in pdf1]
  pdf2_chunks = [chunk for chunk in pdf2]
  pdf3_chunks = [chunk for chunk in pdf3]
  chunks = pdf1_chunks+pdf2_chunks+pdf3_chunks

  embedder = Embedder()

  embeddings_path = Path("./books") / "text_chunk_embeddings.pt"
  if embeddings_path.exists():
    text_embeddings = torch.load(embeddings_path)
  else:
    text_embeddings = embedder.compute_embeddings(chunks)
    torch.save(text_embeddings, './books/text_chunk_embeddings.pt')

  retriever = Retriever(embedder)
  query = "What are the benefits of napping?"

  rag = RAGPipeline(text_embeddings, chunks, retriever, prompt_manager, llm)

  rag.answer_question(query)

  return

!huggingface-cli login

main()

